# Breaking Software:
Although our goal isn't to tear the developers apart, we do want to tear the system apart. 
<br>
This section is all about knowing which things to look for when testing, because they can produce unexpected errors and break the system if not handled correctly.
<br>
<br>
I'll present them as "look for x when testing", but obviously, you could/should "weaponize" these as inputs in your testing efforts to ensure that the system is robust and exits gracefully.

## Logic errors and Operators:
Could be as simple as using a less-than sign (<) instead of a greater-than sign (>), or could be more complicated to the point of using multiple variables and integrations into different systems.
<br>
Usually, I check for operators, and the overall "flow of logic" (In other words, are they doing certain things before they should? Does it make logical sense?)
<br>
<br>
Some errors that the common (usually beginner-level) Dev/Programmer makes:
<br>
Using "break" instead of "continue" within a loop - causing iterations to be skipped.
<br>
Or not using "break" statements in a Switch's Case condition.
<br>

## Off-By-One errors:
These are pretty much logical and Operator errors, but I'll give them their own short section.
<br>
<br>
Using this example from my "Basics of Testing" section:
<br>
<li>The system checks for "if input is <0 show a green light; if input >0 and <50 show a red light" </li>
<br>
We can see that 0 is not covered in this logic, even though it's explicitly mentioned in the code..
<br>
So when we pass in a 0, we don't know what will happen, because the behaviour of the system is "undefined" 
<br>
<i>Note: Logically, we know that no light will show - since nothing will happen for this input as it falls outside of both cases, but in more complex systems, this could have some serious consequences that may not be as easy to spot or understand </i>
<br>
<br>
In this case, the dev should fix it in either of these ways, to include 0 in the logical condition:
<br>
<ul>
<li>"if input is <0 show a green light; if input >=0 and <50 show a red light"</li>
<li>"if input is <=0 show a green light; if input >0 and <50 show a red light"</li>
</ul>
<br>
This is, obviously, a very basic and low-risk example. 
<br>
If this were an error in a financial institution, however, the dev would probably be fired into the sun for losing (potentially) millions of dollars.
<br>
<i>Spoiler alert: These errors DO happen in financial institutions across the globe.</i>


## Rounding and Float errors:
Floats are used to store floating point numbers (A number with a decimal point - like 2.10).
<br>
They're great! ..Or are they?
<br>
<br>
Floats have a pretty dangerous down-side, being that they are don't have a high degree of accuracy. As the precision of the number goes up, the accuracy of it goes down. 
<br>
Storing 2.10 into a float doesn't guarantee that the computer will store it as exactly "2.10", it may store it as 2.10893759, or even 2.99999999912395
<br>
This is fine for a few cases though, and you'd probably think "meh, that's not so bad though!", 
<br>
but when you start to multiply floats x floats, you sometimes get unexpected results, because "what you're inputting" is different to "what the computer is seeing".
<br>
Having this randomness in core Systems, like Banking Systems, could be catastrophic and lose the organization millions of dollars.
<br>
<br>
Always check for the use of float datatypes in important systems and make sure they're being handled correctly.
<br>
<br>
When converting numbers with precision (2.10) into integer datatypes (2), you'll have to round up/down to the ceiling/floor of the float number and lose some data (the ".10").
<br>
Always check that this loss is intentional and insignificant in the context of the system.

## Integration points:
Any integration point between any systems will always have some room for failure, usually in the JSON payloads they exchange (Missing or Bad data will be great to test here).
<br>
There are bound to be errors or defects on either side of an integration - whether that integration is into another internal sub-system, or an external organization.
<br>
Always check these and the integration specifications to ensure that nothing was misinterpreted on your teams' side.
<br>
<br>
Closer to home, integration points can also be the "interfaces" between internal classes/methods - where methods of either class are being called/used.
<br>
These can also be misused and spawn all kinds of logical errors. 
<br>
It's often worth it to read through the code and check the usage of classes/methods (the "flow of logic", as I said earlier) to vet it for any errors.

## System Context and requirements interpretation:
BAs define at a high level what the system should do, and Devs create the system,
<br>
But, because Requirements are generally abstracted, they are left to interpret some things.
<br>
Sometimes, their interpretations are incorrect.
<br>
<br>
Some common things to check, to ensure that the dev interpreted correctly (and to make your testing easier) are:
<ol>
<li>How should the system display errors</li>
<li>How should data be displayed</li>
<li>What format should input/output files be in?</li>
<li>What other systems does this one integrate into, and which interfaces does it have?</li>
<li>What UI is expected?</li>
<li>Who will access this system? How?</li>
<li>What are the expected ranges/limits/volumes of data flowing through this system?</li>
<li>What format should input data be in?</li>
</ol>

## Missing Data:
Sometimes, the entire system can crash or grind to a halt if certain data isn't found where it was expected.
<br>
This could be from some files that need to be in specific formats - where they don't have required fields or contain only spaces which break the format, 
<br>
Or some JSON tags that are missing because the one side of an integration point thought it was unnecessary.
<br>
<br>
Does the system correctly handle not receiving this important data? Does it gracefully drop the current request and move on, or does it stop and keep it in memory? Does it completely crash and spit out hundreds of error messages?
<br>
These are all things we need to test and raise with the project team if the system is not behaving in the expected way


## Bad Data:
Bad Data is data that is unusable by the system for the current operation. 
<br>
For example, if I have a database with a strict column limitation on the ID column, that says "ID can only be a number", and I pass in "test123". 
<br>
The system wont be able to insert that data into that column, because it's not a number - which means that the data ("test123") is bad and the system can't use it.
<br>
Bad Data can either be too long or short, the incorrect format, out of range, unparseable or corrupted.
<br>
<br>
How does the system operate when it gets bad data? Does it correctly exit the current operation? Does it accidentally reveal important info about the internal workings of the system? Does it completely crash and spit out hundreds of error messages?
<br>
<br>
The easiest way to test this - rather than spending hours crafting specific payloads - is to do <b>Fuzz Testing</b> (Basically just generating random automated inputs)

## Injections
We should fear injections like toddlers do.
<br>
Injections are where executable code is given as an input to the system, and the system is tricked into actually executing it.
<br>
This doesn't sound too bad, but it really is.
<br>
<br>
Basically, hackers are able to gain access to your systems, delete/add/edit/read confidential data and hold your system hostage.
<br>
Always test for injections - the most common are SQL injections and LDAP injections, but there are also buffer overflow and integer overflow attacks as well (Where hackers will put assembly code at the end of a very long payload to trick the system into executing the assembly when it crashes).
<br>
If you aren't in charge of Security Testing for your team, at least ensure that the Security Tester ("Ethical hacker" or "Penetration Tester") tests for these conditions.
<br>
<i> Side note: I'm going to have an entire section dedicated to the basics of Security Testing, so don't stress if you don't know it offhand right now</i>

## Display Errors
These are just simple display concerns. Is the system showing this window correctly? Are we using the right images? Is the data accurate? Is there an unescaped HTML element that's breaking the entire view?
<br>
We can test all of this by inserting super big or super small values as inputs to the system and checking that it handles it correctly, and by consulting the Business Requirements.

## Network Connectivity:
Of course, context is King here,
<br>
But some questions to ask ourselves are: 
<br>
Does the system crash when the network is down? Do we lose data if we unplug the ethernet cable and plug it back in? Does the entire system become inoperable and erroneous when the network connection is poor? What happens when we're pulling reports via UDP, instead of TCP?
<br>
<br>
The only ways to test this is via a <b>Hatchet Test</b> (basically just unplugging the network cable, or adding incredible distortion to the network connection)
<br>
In all cases, the system should gracefully slow down until a new connection is established, or should go into a "pending" state, rather than just crashing entirely. 
<br>
Most UI elements should also still be interactable (Of course, within the context of the system), to limit customer frustrations of an unresponsive system!

## Disk Errors
"Risky Code" (in the context of Testing) is code that may or may not fail. We know that it should work, but it COULD fail.
<br>
<br>
An example: Reading a config file from disk.
<br>
It should 100% work, because the system needs that config file, 
<br>
But what if it's not there when the system checks? What if it's been locked out by another process? What if a user has opened it in Notepad and has accidentally changed the format? 
<br>
<br>
What should the system do in these cases?
<br>
<br>
As the tester, we need to verify if there are (actual code) errors when reading from disk and raise these with the project team for the BA and Dev to discuss.
<br>
Note: We need to verify that it's actually a code error, and not an operational/testing error.
<br>
What's the difference?
<br>
Well, a code error is "the dev didn't cater for this failure in the code, so the entire system crashes when the file is locked by another process",
<br>
An operational error is "the tester deleted the entire directory of the config file, and now the system can't find the file at all".
<br>

## Null-Pointer exceptions:
These are basically where the app has tried to use "something", but found "nothing" instead. It was expecting to Square Root "300", but instead found <i>nothing</i>
<br>
<br>
Strongest English enjoyer vs Weakest Dev:
<br>
<ul>
<li>In English: Null is synonymous with "0", because both mean "nothing". Therefore: 0 = null </li>
<li>In Programming: Null literally means "nothing", and "0" DOES NOT mean "nothing", it means "0" (the number 0 - not the concept of "nothing" like in English).</li>
</ul>
Bit weird, I know, but basically: 
<br>
0 is a value (natural number) that we can use in arithmetic or logical arguments. 
<br>
NULL is literally "nothing". It's an undefined portion of the computer's memory - it can also be used in logical arguments, but it's NOT a value, it's literally just nothing.. completely blank!
<br>
<br>
For logical checks, these identical functions do 2x different things:
<br>
```C#
x = 0;
if (x > 0) //Business Requirements: x MUST BE greater than 0 to continue
{
   //[Success] only values of 1 or greater reach this point 
}
```
<br>
```C#
x = 0;
if (x != null) //Business Requirements: x MUST BE greater than 0 to continue
{
   //[Failure] anything that has any data (even 0) reaches here - because 0 doesn't mean "nothing" in programming, it's a value - a piece of data
}
```

In most cases, you will either get a null-pointer exception from the app under test, or you will spot these issues when reading through the code

## Distributed Systems: bane of a Tester's existence
Testing distributed systems is a nightmare, and I can't cover every aspect of how to test one, because no 2x are the same.
<br>
Usually, as a tester, you won't have to do this - and you wouldn't do it alone, but here are some checks that should be done right off the bat:
<ol>
<li> Connectivity to the other parts of the system (Check that the system has access to other servers and services if it needs them)</li>
<li> Time and Timestamps: In a lot of cases, systems need to have time synchronicity, check that systems are synced with time</li>
<li> Data consistency: One system may have the only "true" copy of data, or none may have a "true" copy, and may be different to the others. Ensure that data is consistent across all the servers</li>
<li> Server failure: It will happen. Check that the other servers aren't impacted when one goes down.</li>
</ol>


## Config Errors:
These could be both on the server and client side, but I'll touch on the client-side first:
<br>
<br>
If you have a web-portal, does it still operate as expected when the user is using a different operating system? What happens when they have JavaScript disabled? What happens if they're using Internet Explorer (RIP)
<br>
<br>
Secondly, has the server been configured properly? Are all the config files in a usable state by the system? Does the system have every config it needs? Are any other apps changing configs?
<br>
Has an admin changed settings? Does the system settings impact the client's view of the system?
<br>
And (worst case scenario) are we revealing important info back to the user when the system crashes on config errors?
<br>
<br>
As the Tester, you need to investigate all of these possibilities - either directly, or collaborating with the Developers and admins.

## Domain Specific issues
Of course, context is always King.
<br>
You will experience completely different challenges when testing a nuclear reactor management system, than when you test a children's audio-book reader.
<br>
You'll have lots of random and unheard of issues pop up from time to time, and the best way to manage this is always to consult the domain-experts.
<br>
They will guide you and help you establish a baseline of what could go wrong, to help create new (and more specific/tailored) test cases for the system under test.