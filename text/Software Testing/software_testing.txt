# What Software Testing ISN'T:
<ol>
<li> A way for testers to tear apart a developer's code or debase their skills/abilities</li>
<li> Looking for every single bug and building thousands of automated cases to run for every possible scenario</li>
<li> Pressing every button randomly until something breaks</li> 
<li> Something you do after building everything</li> 
<li> Hoping something will break </li> 
</ol>


## Why do Software Testers exist: Guarding against Personal Interest
The people doing the most collaboration or communicating over the course of the Project are Developers, Business Analysts (BAs), and Project Managers (PMs).
<br>
BAs may try to "get as much requirement gathering done as fast as possible" to get more rapport as being "the fastest/best BA". 
<br>
Devs may try new functionality/apps/languages/libraries to increase their own personal market value (or marketability), 
<br>
And PMs want get the project done as fast as possible to boost their own rapport. 
<br>
<br>
Each of these people have their own personal interest, which affects the Quality of the Software - and none of them are advocating for what the Customer/User wants or needs.
<br>
The Tester keeps them in check by ensuring that Quality is upheld, and that they are creating usable software that meets the Business Requirements.


## Why do Software Testers exist: The Project Team itself
The project management team (as discussed <a href="#" onclick="updateBodyText('https://raw.githubusercontent.com/Cyber-Finn/Blog/main/text/SDM/SDM.txt');">here</a>) consists of the SDM, Tech Lead and PM, but the actual Project team (people doing the work), consists of:

<ol>
<li> A Business Analyst (BA) that gathers requirements from the customer/user.</li>
<li> A Developer that builds an application that meets the requirements of the user.</li>
</ol>
That's perfect - right? We've covered all aspects of Requirements Gathering and Development!
<br>
Well, no. 
<br>
How do we know the Dev will actually build the right thing? (Remember that there are sometimes massive budgets - of hundreds of millions of Dollars - sunk into each project)
<br>
How do we check this, and how do we hold them accountable if they've built subpar software?
<br>
<br>
This is where the tester comes in.
<br>
The tester acts as a bridge between the Developer and the Business Analyst, and consultant/advisor to the Project Management team.
<br>
In a nutshell: The BA gathers requirements, the Dev builds a solution that meets those requirements, and the tester checks that those requirements have actually been met.

## Why do Software Testers exist: Project Risk Assessment:
Business Stakeholders are the least technically-skilled, as all managers are.
<br>
This poses a challenge to them when they try to understand "how far along" a project is, how good the software will be, and if there are any risks involved with releasing the project (Their roles focus on financial/reputational damages from the release of buggy software, how many customers they'll lose, market share values, etc.).
<br>
While Stakeholders don't really care about knowing the exact quality of the software, because they aren't technical and don't have time to read through the dev's code - they do care about the overall risk of releasing the software.
<br> 
So, instead of having them read through the code, the Tester independently reports to these stakeholders on the test plan, test coverage, bugs identified, severity of bugs, etc., to help them understand what risks there are and how big they'll be. 
<br>
Ultimately, the tester doesn't (and shouldn't ever) have the power to halt or scrap projects, but they do have the authority to advise the Project management team of issues and risks.
<br>
Their job doubles as both an "Advocate for the User" and "Advisor to Business Stakeholders".


# What do Software Testers actually do?
In addition to verifying that the Business Requirements were met and reporting project quality to project management,
<br>
They perform 2x very specific functions. These are:

<ol>
<li>
Validation
<ul>
	<li>This is where they look at the app the dev has built, and the requirements the user has requested, and check that they're related.</li>
	<li>For example, a customer may have asked "I need a basic percentage calculator", but your dev has built "AES256 advanced encryption for logins using an Identity Provider service that auths logins via Kerberos Authentication on port 80"</li>
	<li>This is a very exaggerated example, but devs do often go out of their way to build something they want instead, because it helps their CVs/Resumes or makes their lives more interesting.</li>
</ul></li>
<li>
Verification
<ul>
	<li>This is where they will look at <i>how the app has been built</i>. </li>
	<li>They ask/test questions like: Does it have enough logical error handling? What happens if we pass in a string instead of a number? How well does it perform under load/stress? What happens if memory is exhausted? etc.</li>
	<li>They'll basically test the code that the dev has written, to verify that it's of acceptable quality and is ready for market.</li>
	<li>This type of testing can be split up into different approaches, like black-box testing, manual vs scripted/automated, regression testing, etc. </li>
</ul></li>
</ol>
<br>

# Defects vs Enhancements
This one is all about the Customer Experience (AKA User Experience or UX).
<br>
A customer may use these interchangeably, but they are completely different.
<br>
A Defect is: defective software that does not behave in the expected way -> basically, code that doesn't meet the business requirements, or breaks the system when certain actions are performed.
<br>
An Enhancement is: anything a customer requests to be added or reworked in their favour, when the existing software/system is actually still operable. If there are "manual workarounds", they are asking for an enhancement - not reporting a defect.

## Why test Software in the first place?
Well, Software Testers are important because they catch defects early into development
<br>
Thereby reducing cost of rework, and assist stakeholders with project risk profiling, leading to reduced financial losses from mistakes.
<br>
<br>
Testers also bring stability to the system by increasing test-coverage of the system. 
<br>
This makes it easier for future Devs to pre-emptively know when they've broken something important during an upgrade because they will see that tests are failing before deployment. 
<br>
This allows them to investigate and resolve the issue in-one-sitting, rather than having a client report it weeks later and having the issue go onto the backlog.
<br>
<br>
Over time, Testers also develop a more holistic overview of the entire system in completion, over the devs who have scope of only a select few areas, so Testers can actually help guide Devs on which things might break during their changes/fixes and reduce the overall impact of these changes. 
<br>
This is also a function that Testers should perform in scope of their jobs. If you're aware of issues that will arise after a dev does a fix, raise it beforehand and work with the dev to pre-emptively resolve the issues.
<br>

## Can Devs "Mark their own homework": From the Developer perspective
Absolutely!
<br>
Devs should 100% test their own code while they write it. 
<br>
This helps them to write testable code and keep the software closer to the Business Requirements.
<br>
<br>
It's still VERY important to have a designated Software Tester, and to deeply involve them in the project, 
<br>
Because (as the Dev) it protects you when things go incredibly wrong and Management are (literally) head-hunting.
<br>
<br>
Testers (thankfully) are also the ones that have to write testcases for all your code
<br>
So you (as the dev) don't need to spend countless hours dreaming up every scenario and writing <i>code to test code</i>!
<br>
You can simply test your code as best as possible in that moment, cater for as many different scenarios as you can think of, 
<br>
And leave code-coverage and exhaustive-testing to the designated Tester.
<br>
<br>
For the dev, it's also INCREDIBLY GOOD to have someone independent, qualified and trusted that double-checks your work and gives the thumbs-up.
<br>
It helps establish (to the larger crowd/group/team) that you're not only dependable, but that you also write <b>Good Quality Software</b>.
<br>
Working closely with the tester, and making them happy, is a sure-fire way to get your skills recognized. The Tester is your best friend!
<br>

## Can Devs "Mark their own homework": From the Business perspective
Absolutely not!
<br>
Devs should 100% not be trusted. They lie, embellish and overestimate their own abilities because of fanaticism or egoism.
<br>
Because Software Development is an intellectual enterprise, they embellish their skills or certain events to make themselves more attractive/experienced/trustable,
<br>
Because it makes them feel more important or like <a href="#" onclick="updateBodyText('https://raw.githubusercontent.com/Cyber-Finn/Blog/main/text/The%20Shamans/the_shamans.txt');">"Shamans" or Wizards</a>.
<br>
<br>
Developers (Specifically self-proclaimed "Software Engineers") will always do what they want, when they want, if they're left to their own devices, to help their own marketability and resumes/CVs.
<br>
<br>
To combat this, we've hired Project Managers that <i>ensure that work is being done</i>. 
<br>
But who ensures that the work being done <i>is actually the work required</i>?
<br>
Who checks that the software is usable, that it won't make us the laughingstock of Wallstreet? 
<br>
How do we know that it's "good quality" or can make us competitive?
<br>
<br>
As "Business", we need someone that verifies Quality, ensures that Devs aren't "taking us for a ride", and reports any major issues with the project back to us, 
<br>
So that we can prepare to manage risk/damages.
<br>
That person is the dedicated Software Tester, who acts as our proxy and advisor.
<br>


<br>
# Different Types of Testing:

## What is Regression Testing, and how does it differ from "regular" testing?
"Regression testing" is one of those keywords that everyone knows, but nobody actually knows what it means.
<br>
Simply put, we go back and re-test EVERYTHING (we "REGRESS" to the first test cycles we did for this software). 
<br>
That's all it is. We rerun every test against the code.
<br>
It’s done after bug fixes, new features were added, or performance improvements were made, to confirm that the existing/old software still works as expected.
<br>
This is usually done via automated scripts that we've developed over the course of the SDLC for this piece of software.

<ol>
	<li> Purpose
		<ul>
			<li>Regular Testing: Focuses on verifying new features or functionalities to ensure they work correctly</li>
			<li>Regression Testing: Ensures that existing features continue to function correctly after changes are made</li>
		</ul>
	</li>
	<li> Scope
		<ul>
			<li>Regular Testing: Typically targets specific new features or components.</li>
			<li>Regression Testing: Covers both new and existing features to detect any unintended side effects of recent changes</li>
		</ul>
	</li>
	<li> Frequency
		<ul>
			<li>Regular Testing: Conducted during initial development phases or when new features are added</li>
			<li>Regression Testing: Performed continuously throughout the development lifecycle, especially after any code changes</li>
		</ul>
	</li>
	<li> Automation
		<ul>
			<li>Regular Testing: Can be manual or automated, depending on the complexity and requirements</li>
			<li>Regression Testing: Often automated to efficiently retest large parts of the application</li>
		</ul>
	</li>
</ol>