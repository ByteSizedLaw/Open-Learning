# The root of all testing: 
Create some inputs, define the expected outputs the system should give you, then execute the test and check if the results match our expectations for this input.

## Equivalence classes and Test Partitioning:
<i>Equivalence class: Basically one set of inputs that generate the same output </i>
<br>
For example, imagine we have the following inputs: "-10", "abcd", "0", "1000"
<br>
being passed into a Square-root function. Since they're all Strings, they all generate the same "not a number" error.
<br>
In our example, we realized that, although we have 4x test cases, they're really only 1x test case, because we've only tested passing in a string..
<br>
We haven't tested anything else.
<br>
<br>
Realizing this, we know to be aware of over-testing specific parts of our app or test-cases - while not testing others at all. (like in our example, where we tested only strings, but no negative numbers, etc.)
<br>
How do we check this though? How do we know that we're not over-testing or re-testing the same condition?
<br>
We need some way of PARTITIONING our test case inputs, to check that they don't overlap or re-test the same inputs. 
<br>
We also need some way of reducing the number of test cases we have to write, because exhaustive testing can take ages to perform - and may yield no new information over some basic (and well optimized) tests.
<br>
This is where Equivalence Class Partitioning comes in.
<br>
<br>
Basically, we just ensure that our tests don't overlap with each other, and cover all respective inputs so that we're properly testing the system, 
<br>
and not falsely reporting that "the system passed 5000 test cases! :D" when it's only really passed 1 test-case, which was "what happens when it gets a string instead of a number".
<br>
<br>
Strict partitions are good, because it maps a single input to a single output type.
<br>
Strict partitions (with the tester and Dev working together to improve the system during testing) help prevent cases where (for example) the square-root of 4 produces both the number 2 and "abcd".


## Expected behaviour vs Observed behaviours:
I think this one is pretty self-explanatory, so I'll just give a recap:
<br>
Expected behaviours are the behaviours of the system, as defined in the business Requirements. For example: "System prints abcd when user inserts ABCD".
<br>
Observed behaviours are the behaviours we've seen the system display during our testing. For example: "System prints error-invalid-memory-address when user inserts ABCD".
<br>
This observed behaviour (in our example) is totally different to what we've expected.
<br>
<br>
Testers may also sometimes find an "undefined" behaviour - which is basically where a test falls outside of a logical error "range".
<br>
For example, "if input is <0 show a green light; if input >0 and <50 show a red light"? 
<br>
Would passing in 51 now produce a light or no light? What would passing in a 0 do? I couldn't tell you, because the system doesn't cater for it - but it probably should, right?
<br>
<br>
As the Tester, obviously, you need to test this scenario, but also make the developer aware of these scenarios to help robust the code and improve the system.
<br>
Working with the Dev and BA to resolve "undefined" behaviours will improve the system, test coverage, and make your job as the Tester a lot easier!
<br>


## How to choose Inputs that maximise the probability of finding defects:
Most defects will be found near boundaries of Equivalence classes -> basically, at the end of one Equivalence Class, and the start of the new one.
<br>
Why? because they're often mentioned in code (if x >= 1) [1 is a boundary value here]
<br>
<br>
Boundary values: are found at the Start and Ends of Equivalence Classes.
<br>
Interior values: every item within a given equivalence class - except the start and end values.
<br>

## Explicit boundaries vs Implicit boundaries
Explicit boundaries are those defined by the business requirements, and are runtime independent.
<br>
Implicit boundaries are runtime and context dependent. They're usually either test-cases that fit the given nature of the application, or conditions that the BA didn't consider when writing the requirements. 
<br>
For example, how much memory is available? Is the server even on? How much data can the system process at once? etc.
<br>
<br>
Basically:
<br>
Explicit boundaries are literally, explicitly stated by the BA as being a requirement the system has to have/pass.
<br>
Implicit boundaries are not explicitly stated, and the tester needs to think of their own inputs, based on the context of the application.
<br>


## Base cases, Edge cases and corner cases:
Base: refers to both normal input parameters, and where the System operates as expected
<br>
Edge: refers to both abnormal input parameters, and where the System operates erroneously
<br>
Corner: refers to both ridiculous/illogical input parameters and/or where multiple errors occur simultaneously
<br>

## Happy Path vs Rainy Day Testing:
Happy path: We test with valid inputs and the system operates as expected.
<br>
Rainy day testing: We test the worst-case scenarios and actively try to make the system fail. We do this to verify that it's producing the correct outputs and error messages, and that it actually can even survive worst-case conditions!
<br>

## Test case outputs:
There are only 2x types:
<br>
Positive: system returns expected result (These are usually the actions the user would take under normal conditions - and should always result in positive responses)
<br>
Negative: system didn't return the expected result due to failure or some error (These are usually cases we expect to fail, like trying to square root a name)


## Blackbox, Whitebox and Greybox Testing:
Blackbox: where we dont know anything about the internal workings of the system - we emulate the user.
<br>
Whitebox: we know everything about the internal system and even have its code.
<br>
Greybox: we know a bit about the system and how it works, but not everything. we're somewhere between Blackbox and Whitebox testing.
<br>


## Static and Dynamic testing:
A way of grouping tests - like we did for partitioning, but not exactly.
<br>
We basically group our tests into "what are we going to test on the actual app itself" and "what are we going to investigate/verify on the code itself".
<br>
<br>
Dynamic tests are run on the system itself. We actually run the code and give it inputs, etc.
<br>
For example, to test a button, we'd need to actually click that button - but that requires that we're actually running the system/app, to be able to press the button.
<br>
<br>
Sometimes, in some cases, this can be dangerous - if the system operates with some very important data and the only way to test it is to actually run it in production, because you could accidentally break production, etc.
<br>
So we also have Static testing at our disposal, which is basically where we can run Linters (they're software libraries that scan source code for errors [similarly to how your IDE always knows when you've not declared a variable, or used the wrong syntax]) or where we actually review the source code and check it for logical errors.
<br>
<br>
Static testing aims to:
<ol>
<li>Reduce the impact of testing (or straining resources) on important systems</li>
<li>Look at the code itself, rather than the results of using that code.</li>
</ol>

let's stop there for a second.
<br>
This isn't a "Static vs Dynamic testing" argument. 
<br>
Both are incredibly important, and BOTH should be used, because both focus on 2x very different aspects of the system.
<br>
<br>
Dynamic testing: focuses on using the actual system and seeing that it operates as expected (Operational Analysis).
<br>
Static testing: focuses on the actual source code and checking its' quality (Quality Analysis).
<br>
<i>Side note: in case you missed it, Code Reviews are actually an example of Static Testing!</i>


## Business Requirements and Testability:
Right off the bat, requirements that cannot be tested should be scrapped immediately, because they are vague or "umbrella requirements".
<br>
Remember, Business are sinking (potentially) millions of dollars into this project - how are you going to prove to them that "our new social media platform is curing every cancer patient that uses it" if you have no way of knowing that for sure?
<br>
You need clear metrics to measure and prove that requirements have been met.
<br>
<br>
If it were up to Business, the ONLY requirement would be "REQ-1: Make 100% more profit, every day.",
<br>
But this is completely unrealistic, and makes your job (as Tester) near impossible. 
<br>
How do you define testcases for this lofty business goal? Which system do you need to test? Which inputs can it take? It just doesn't make any sense!
<br>
<br>
How do we combat this?:
<br>
All BAs should involve Testers when they do the official Business Requirements specifications. 
<br>
This allows the Tester to determine upfront if they're able to test the requirements, or if the requirements are still too vague and need to be broken down further.
<br>

## Writing Business Requirements:
I'm going to have a whole section on this in the Business Analysis part of my site, but for now, I'll include this here:
<br>
Testers are rarely ever required to write tests - and usually only when there isn't already a BA - but it's still good to know how to write them, so that you can help the BA rewrite the ambiguous ones!
<br>
<br>
Firstly, lets quickly discuss the 2x types of Business Requirements.
<br>
They are: Functional (what a system should <i>do</i>) and non-functional (how the system should <i>perform</i>).
<br>
Because "performance" is hard to test/quantify, all requirements (where Testers are involved) should be Functional Requirements, because they are testable.
<br>
<br>
For Testers, all requirements should be Functional Requirements and:
<ol>
<li>Complete: How do you test the entire system, if you've only got requirements that focus on a specific area? Requirements should cover the entire system, without allowing anyone to interpret anything.</li>
<li>Consistent: How do you test requirements that contradict each other like "when the number of items in the cart is 100, make it 0" and "when the number of items in the cart is 200, give them 20% discount"?.</li>
<li>Clear: How do you interpret "the camera scans the user with the red lens"? Is the "red lens" the user's lens, or the camera's, and which camera are we talking about?</li>
<li>Measurable: How do you test "when the user logs in, the system responds QUICKLY"? How fast is quick? How can I emulate/change what their network latency will be at the time of login? Will they login from overseas?</li>
<li>Possible: How do you test "10 trillion users logging in at once"? you don't. These kinds of requirements need to be removed or rethought.</li>
</ol>


## The basic structure of a Test Plan:
A test plan is basically just a list of test-cases.
<br>
Each company, software tool and team will have their own way of doing things, but the general template is something like:
<ol>
<li>Test Identifier: Something like "10", "DB-1" or "SQL-Injection-Test". Basically any name that acts as a unique ID for this test case</li>
<li>Description: of the test case and what it's testing</li>
<li>Prerequisites: Anything needed before the test can commence (Remember, you may need other testers to pick this document up and start testing. This tells them what to do before they do the actual test!)</li>
<li>Inputs: any input values we need to feed the app for the test</li>
<li>Execution or Replication steps: What the user needs to do (step by step) to perform the test. (Again, remember that other testers may need to do this, so tell them, in detail, how to test!)</li> 
<li>Output: any output values we got from the app as a result of the test.</li>
<li>Postconditions: basically anything that's happened as a result of the test (but that isn't an output/response). For example, data on the Database has been changed, a new service instance has been spawned on the server and needs to be manually stopped, etc.</li>
</ol>
<br>
Note: Preconditions are less "responses" (like outputs are) and more "as a result" of the test. Like I mentioned, they could be services getting spawned, database updates/changes/deletions, audit alerts, IP bannings, etc.
<br>
In some cases, you may need the Tester to go and "undo" these conditions in some way to allow for future testing and limit impact to the system.



## Easiest way to write a test plan:
Use the Business Requirements and just develop test cases for them as you read through them.
<br>
Of course, context is King. Your detail and effort in creating the test plan for a Nuclear Reactor monitoring system will differ to that of a mobile app game.
<br>
Over-engineering anything is also bad, so try to group requirements together where possible to reduce the sheer number of test cases you could have for more complicated/larger systems, to allow yourself to reuse test-cases for different parts of the system, where possible.
<br>
<br>
You need to (within the context of the requirements and system) balance having enough cases for good test-coverage with minimal testing effort to free up time and resources, for maximum efficiency.
<br>
You also need to be aware of the context within which the system will operate, and if there are any other tests (sometimes completely unrelated to what you should be testing) that you need to do. 
<br>
You can usually chat to some helpful devs or the PM/BA to find out if there's any known issues that may impact testing, and if there's anything specific you need to do to see if the (sometimes completely unrelated) parts of the system will impact this app

## Test Fixtures (or Test Contexts):
They're basically apps or scripts that set the environment up for testing, with simulation of external systems, specific conditions or test data.
<br>
They basically (usually within the QA environment) simulate the conditions that the actual application would operate in, like integrations into McDonalds, having 100 logins at any given time, getting files from a customer, etc.
<br>

## Executing Test Plans:
Doing this is called a Test Run, and produces any of the 6x final statuses (Whether automated or manual testing took place):
<ol>
<li>Passed: The system operates as expected</li>
<li>Failed: The observed behaviour doesn't match the expected behaviour</li>
<li>Paused: Test was started but put on hold for a brief moment</li>
<li>Running: The test is in progress.</li>
<li>Blocked: The test can't be run at the moment. Could be because resources are too busy, equipment is not available, etc.</li>
<li>Error: There's something preventing this case from running - usually because the test case itself doesn't match the context of the app/function. For Example: The user's profile photo is displayed in the chat window, but the app is a basic command-line app that copies files.</li>
</ol>
<br>

## Test Run Tracking: Project perspective:
Every company, team and tool will have its own way of doing things, so do what they do,
<br>
But the general guidelines are to include the following info in your chosen tracking-medium (whether it's Excel, notepad or some custom software):
<ol>
<li>Date</li>
<li>Tester's Name/ID</li>
<li>Name of system/subsystem being tested</li>
<li>Name/Description of the code being tested</li>
<li>The Test plan that this run corresponds with</li>
<li>The Final status of every test case</li>
<li>A list of any defects detected from the test cases</li>
</ol>
<br>
In the case of failures, include the reason why they failes - and link/mention any known issues that this failure relates to. If the issue isn't known, note it down and raise the issue!
<br>
When you encounter "errors" (bad test case), try to reason why it could've happened and raise that with the larger team. Try to get clarity on whether or not it was a good or bad test.
<br>
Errors dont mean that you did something bad, it just means that you need to sit with the BA and Dev and understand if the test is relevant, or can be excluded - or if there's new dev that needs to be done to cater for it (Since you would've followed the business requirements to the T, when designing test cases).
<br>

## The Traceability Matrix:
How do we know that our Test Case matches the Business Requirement?
<br>
How do our readers (management reading our test plan and test run results) know which cases test which requirements?
<br>
Also, how do we know if we have a test-case for every Business Requirement, and how do we know if we have test cases that dont match to business requirements?
<br>
<br>
The Traceability Matrix allows us to do all that!
<br>
<br>
"Tracability Matrix" is just a fancy phrase for saying "This requirement is tested by: 1, 2, 3, 7, 8".
The format is also basically just that! 
<br>
It's "Name of requirement" + Colon (the : character) + "Test number or Unique ID". 
<br>
Each test case is separated by a comma, and every requirement is listed underneath the others in this way.
<br>
<br>
For example:
<br>
FUN-1: 1,2,6,7,8
<br>
FUN-2: 3,4
<br>
FUN-3: 5,9,10
