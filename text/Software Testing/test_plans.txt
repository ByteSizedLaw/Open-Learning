# Test Plans:
Here's where we walk through creating test plans.
<br>
Pre-requisite to the Test Plan: You already noted the expected behaviours of the system's Requirements, 
<br>
Through discussing each of the Business Requirements with the BA.

## Plotting the Test:
My approach is to run through the requirements in a top-down fashion, as they're stated in the Business Requirements Specifications.
<br>
Reading through them, I like to group them all together in terms of:
<ol>
<li>Inputs</li>
<li>Outputs</li>
<li>Functionality</li>
</ol>
These help us immediately know what we're testing on each of these requirements, 
<br>
But also allow us to start devising test cases for each of them - we may also be able to reuse test cases for different requirements, like the Inputs, etc.
<br>
It also tells us where the bigger portion of the app is, because we can see which section has the most requirements, and thereby know where most of our testing effort should go.

## Creating test-cases:
<i>Variants: Variations in test case inputs. I could insert 1234 into a Square Root function, or I could insert a new variant of "hello, world".</i>
<br>
Context is King, so we need to weigh the amount of resources, importance of the app and testing time in each decision,
<br>
But we also need to decide how many <b>variants</b> we want to test for.
<br>
Usually, we'd want to - at the bare minimum - test the Happy Day path, and ensure that the system works as expected for that.
<br>
We'd also want to include some tests for the boundaries, and some basic tests for "missing inputs" (no input) or "incorrect inputs" (strings instead of numbers).
<br>
<br>
As the tester, you're probably thinking "ok, but I need like a million test-cases to know for sure that I've tested everything?", nope!
<br>
You don't need a test-case for every possible input, you only need a few well optimized/targeted ones.
<br>
<br>
Generally, we want to keep variants to a minimum, because it's just testing the same thing - and we already know what will happen when the test is run.
<br>
<br>
There's no point testing "what happens if I try to square root the name Jack" and "what happens if I try to square root the name Jill", because both are literally the same test.
<br>
<br>
As the tester, to maximise efficiency, you want to test as little as possible - and only focus on the key, pressure points of the system. 
<br>
Still test thoroughly, but don't write a billion test cases!

## Creating Equivalence Classes
Once we've got focus of "which requirement cluster will be the hardest/most to test", we can start to draw up test cases for them.
<br>
We did this by looking at the explicit boundaries of the Business Requirements, and partitioning them into (Equivalence Classes) different groups that test different things.
<br>
We then also add a few test-cases for the boundaries of Equivalence Classes - so that we're testing what happens when data is inside and outside of ranges.
<br> 
We then add these all to our Test Plan, since we've now got a pretty solid Test Plan in place.
<br>

## The basic structure of a Test Plan:
A test plan is basically just a list of test-cases, which we use to see how far along we are with testing.
<br>
Each company, and team will have their own way of tracking test-cases, like using notepad, Excel, or some fancy tool to track the test plan,
<br>
Even these software tools may have their own ways of tracking test plans,
<br>
But in each case, each test-case in the List should, at least, have this info:
<ol>
<li><b>Test Identifier</b>: Something like "10", "DB-1" or "SQL-Injection-Test". Basically any name that acts as a unique ID for this test case</li>
<li><b>Description</b>: of the test case and what it's testing</li>
<li><b>Prerequisites</b>: Anything needed before the test can commence (Remember, you may need other testers to pick this document up and start testing. This tells them what to do before they do the actual test!)</li>
<li><b>Inputs</b>: any input values we need to feed the app for the test</li>
<li><b>Execution or Replication steps</b>: What the user needs to do (step by step) to perform the test. (Again, remember that other testers may need to do this, so tell them, in detail, how to test!)</li> 
<li><b>Output</b>: any output values we got from the app as a result of the test.</li>
<li><b>Postconditions</b>: basically anything that's happened as a result of the test (but that isn't an output/response). For example, data on the Database has been changed, a new service instance has been spawned on the server and needs to be manually stopped, etc.</li>
</ol>
<br>
Note: Preconditions are less "responses" (like outputs are) and more "as a result" of the test. Like I mentioned, they could be services getting spawned, database updates/changes/deletions, audit alerts, IP bannings, etc.
<br>
In some cases, you may need the Tester to go and "undo" these conditions in some way to allow for future testing and limit impact to the system.

## Easiest way to write a test plan:
Use the Business Requirements and just develop test cases for them as you read through them.
<br>
Of course, context is King. Your detail and effort in creating the test plan for a Nuclear Reactor monitoring system will differ to that of a mobile app game.
<br>
Over-engineering anything is also bad, so try to group requirements together where possible to reduce the sheer number of test cases you could have for more complicated/larger systems, to allow yourself to reuse test-cases for different parts of the system, where possible.
<br>
<br>
You need to (within the context of the requirements and system) balance having enough cases for good test-coverage with minimal testing effort to free up time and resources, for maximum efficiency.
<br>
You also need to be aware of the context within which the system will operate, and if there are any other tests (sometimes completely unrelated to what you should be testing) that you need to do. 
<br>
You can usually chat to some helpful devs or the PM/BA to find out if there's any known issues that may impact testing, and if there's anything specific you need to do to see if the (sometimes completely unrelated) parts of the system will impact this app

## Test Fixtures (Test Contexts):
They're basically apps or scripts that set the environment up for testing, with simulation of external systems, specific conditions or test data.
<br>
They basically (usually within the QA environment) simulate the conditions that the actual application would operate in, like integrations into McDonalds, having 100 logins at any given time, getting files from a customer, etc.
<br>

## Executing Test Plans:
Doing this is called a Test Run, and produces any of the 6x final statuses (Whether automated or manual testing took place):
<ol>
<li><b>Passed</b>: The system operates as expected</li>
<li><b>Failed</b>: The observed behaviour doesn't match the expected behaviour</li>
<li><b>Paused</b>: Test was started but put on hold for a brief moment</li>
<li><b>Running</b>: The test is in progress.</li>
<li><b>Blocked</b>: The test can't be run at the moment. Could be because resources are too busy, equipment is not available, etc.</li>
<li><b>Error</b>: There's something preventing this case from running - usually because the test case itself doesn't match the context of the app/function. For Example: We test that the user's profile photo is displayed in the chat window, but the app is a basic command-line app that copies files.</li>
</ol>
<br>

## Test Run Tracking: From the Project perspective:
Every company, team and tool will have its own way of doing things, so do what they do,
<br>
But the general guidelines are to include the following info in your chosen tracking-medium (whether it's Excel, notepad or some custom software):
<ol>
<li>Date</li>
<li>Tester's Name/ID</li>
<li>Name of system/subsystem being tested</li>
<li>Name/Description of the code being tested</li>
<li>The Test plan that this run corresponds with</li>
<li>The Final status of every test case</li>
<li>A list of any defects detected from the test cases</li>
</ol>
<br>
In the case of failures, include the reason why they failes - and link/mention any known issues that this failure relates to. If the issue isn't known, note it down and raise the issue!
<br>
When you encounter "errors" (bad test case), try to reason why it could've happened and raise that with the larger team. Try to get clarity on whether or not it was a good or bad test.
<br>
Errors dont mean that you did something bad, it just means that you need to sit with the BA and Dev and understand if the test is relevant, or can be excluded - or if there's new dev that needs to be done to cater for it (Since you would've followed the business requirements to the T, when designing test cases).
<br>

## The Traceability Matrix:
How do we know that our Test Case matches the Business Requirement?
<br>
How do our readers (management reading our test plan and test run results) know which cases test which requirements?
<br>
Also, how do we know if we have a test-case for every Business Requirement, and how do we know if we have test cases that dont match to business requirements?
<br>
<br>
The Traceability Matrix allows us to do all that!
<br>
<br>
"Tracability Matrix" is just a fancy phrase for saying "This requirement is tested by: 1, 2, 3, 7, 8".
The format is also basically just that! 
<br>
It's "Name of requirement" + Colon (the : character) + "Test number or Unique ID". 
<br>
Each test case is separated by a comma, and every requirement is listed underneath the others in this way.
<br>
<br>
For example:
<br>
FUN-1: 1,2,6,7,8
<br>
FUN-2: 3,4
<br>
FUN-3: 5,9,10